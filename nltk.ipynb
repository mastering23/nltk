{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c29a99f-94ae-4dbb-9000-9463ffe7a503",
   "metadata": {},
   "source": [
    "# Intro NLTK ‚öóÔ∏è\n",
    "- NLTK, or Natural Language Toolkit, is a Python library designed for natural language processing (NLP). It provides tools and resources for tasks like tokenization, stemming, lemmatization, part-of-speech tagging, parsing, and semantic reasoning. NLTK also includes corpora and lexical resources, making it a comprehensive platform for working with human language data. It is widely used in research, education, and industry for tasks such as text analysis, sentiment analysis, and language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd7716-b01a-45d8-ae25-7d81e0e03413",
   "metadata": {},
   "source": [
    "### Stemming method üîç\n",
    "- in Python, particularly within the context of Natural Language Processing (NLP), refers to the process of reducing words to their root or base form (stem). This technique aims to normalize text data by removing suffixes, prefixes, or inflections, thus grouping related words under a common stem. Stemming is used to simplify text analysis, improve information retrieval accuracy, and reduce the dimensionality of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "757095d8-279c-42bf-9bf0-5e46f0bab1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c681c8cf-55ac-442e-9912-3fbf3ba0bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee54be0-5f0b-4a9d-984e-efe56523b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d2a9de1-0498-4f49-ba8e-5c0a83f6b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner', 'ran','runs','running','because','cause','pack','packages','logger','log','login','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c704ddd-abe0-4c19-ae1f-20cd0f475208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run----->run\n",
      "runner----->runner\n",
      "ran----->ran\n",
      "runs----->run\n",
      "running----->run\n",
      "because----->becaus\n",
      "cause----->caus\n",
      "pack----->pack\n",
      "packages----->packag\n",
      "logger----->logger\n",
      "log----->log\n",
      "login----->login\n",
      "easily----->easili\n",
      "fairly----->fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + '----->' + p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7081addb-ccfa-4126-af98-0d6adc915b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48e64137-3dd0-4b10-b1f6-dab6bc7c3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_temmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f837f61-38c2-41bf-815c-b15d3d80832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run----->run\n",
      "runner----->runner\n",
      "ran----->ran\n",
      "runs----->run\n",
      "running----->run\n",
      "because----->becaus\n",
      "cause----->caus\n",
      "pack----->pack\n",
      "packages----->packag\n",
      "logger----->logger\n",
      "log----->log\n",
      "login----->login\n",
      "easily----->easili\n",
      "fairly----->fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + '----->' + s_temmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c38369-b1e6-4c9d-8d9e-7621e9767ba6",
   "metadata": {},
   "source": [
    "# Lemmatization üó≥Ô∏è\n",
    "- In Python, lemmatization is a process of converting a word to its base or root form, also known as a lemma, which is a dictionary form of the word. Unlike stemming, which simply removes suffixes or prefixes, lemmatization considers the word's context and part of speech to ensure the result is a valid, meaningful dictionary word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7f64f08-8b69-4f24-a8be-7c4bb57934dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp =spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1436c786-0d62-4dd4-aaeb-e60f2ff10d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 =  nlp(u'I am a runner running in a race because i love to run since I ran today ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6be4bbcc-4fd0-4b73-8b72-13d117ce6c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON 4690420944186131903 \t I\n",
      "am \t AUX 10382539506755952630 \t be\n",
      "a \t DET 11901859001352538922 \t a\n",
      "runner \t NOUN 12640964157389618806 \t runner\n",
      "running \t VERB 12767647472892411841 \t run\n",
      "in \t ADP 3002984154512732771 \t in\n",
      "a \t DET 11901859001352538922 \t a\n",
      "race \t NOUN 8048469955494714898 \t race\n",
      "because \t SCONJ 16950148841647037698 \t because\n",
      "i \t PRON 4690420944186131903 \t I\n",
      "love \t VERB 3702023516439754181 \t love\n",
      "to \t PART 3791531372978436496 \t to\n",
      "run \t VERB 12767647472892411841 \t run\n",
      "since \t SCONJ 10066841407251338481 \t since\n",
      "I \t PRON 4690420944186131903 \t I\n",
      "ran \t VERB 12767647472892411841 \t run\n",
      "today \t NOUN 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text,'\\t',token.pos_,token.lemma,'\\t',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922fee8-d6f1-4ca0-a428-11b35a32bd10",
   "metadata": {},
   "source": [
    "## üìò Here's what the preview definition shows:\n",
    "- üî§ Root Word | üß¨ Type of Root Word | üß≠ Hash Reference (Pointer) to en_core_web_sm library | üìö References the Root Word from the previously mentioned library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2ac36-5fb6-44cc-8f76-f905f0d3488e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "959bb9f3-cad4-4fb6-99c2-88d2409643cd",
   "metadata": {},
   "source": [
    "## üõë Stop Words\n",
    "- Words like \"a\", \"the\", and \"is\" appear so often üìö\n",
    "- They're not as important as nouns, verbs, or modifiers üß†\n",
    "- These are called stop words and are usually filtered out üßπ\n",
    "- üßæ spaCy includes a built-in list of 326+ English stop words üìò\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e29656a1-371f-45fe-87d8-460d68211ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b82ba6f-d532-47ad-8b4a-67a9ccf32fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a41b24e-0fc5-40d9-bee9-f797b29ed95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wherein', 'thru', 'must', 'being', 'their', 'nine', 'meanwhile', 'themselves', '‚Äòs', 'are', 'among', 'who', 'keep', \"n't\", '‚Äôll', 'she', 'give', 'full', 'amongst', 'it', 'last', 'which', 'various', 'bottom', 'enough', 'off', 'because', 'few', 'also', 'the', 'all', 'as', 'around', 'sixty', 'somewhere', 'otherwise', 'please', 'do', 'n‚Äôt', \"'s\", 'almost', 'each', 'this', 'am', '‚Äôve', 'twenty', 'wherever', 'therefore', 'hers', 'fifteen', 'thence', 'perhaps', 'five', 'just', 'often', 'when', 'mostly', 'latter', 'over', '‚Äòll', 'although', 'someone', 'yours', 'now', 'no', 'more', 'made', 'latterly', 'due', 'elsewhere', 'most', 'during', 'thereby', 'anywhere', 'onto', 'behind', 'were', 'anyone', 'either', 'go', 'whereas', 'get', 'well', 'under', 'make', 'next', 'though', 'top', '‚Äôre', 'in', 'done', 'neither', 'into', 'never', 'ever', 'everything', 'here', 'nevertheless', 'he', 'least', 'unless', 'anyhow', 'hundred', 'used', \"'ll\", 'out', 'within', 'upon', 'itself', 'across', 'him', 'before', 'two', 'nothing', 'its', 'very', 'therein', 'our', 'beside', 'call', 'less', 'ca', 'both', 'you', 'further', 'n‚Äòt', 'ten', 'mine', 'an', 'own', 'his', 'where', 'they', 'than', 'together', 'how', 'since', 'yourself', 'became', '‚Äòd', 'hereafter', 'every', 'on', 'same', 'show', 'by', 'formerly', 'through', 'always', 'becomes', 'whenever', 'doing', 'move', 'anyway', 'per', 'cannot', 'there', 'everywhere', 'serious', 'below', 'may', 'me', 'thus', 'we', 'any', 'only', \"'ve\", 'rather', 'himself', 'does', 'some', 'yourselves', 'twelve', '‚Äôs', 'put', 'have', 'besides', 'that', 'and', 'i', 'about', 'toward', 'even', 'already', 'whereupon', 'take', 'eight', 'forty', 'else', 'much', 'against', 'afterwards', 'throughout', 'using', 'be', '‚Äôd', 'via', 'whether', 'with', 'namely', 'of', 'above', 'did', 'could', 'what', 'nobody', 'alone', 'can', 'beforehand', \"'re\", 'myself', 'whence', 'none', 'many', 'whole', 'herself', \"'d\", 'a', 'then', 'has', 'quite', 'until', 'moreover', 'too', 'sometime', 'except', 'whose', 'indeed', 'not', 'ourselves', 'seeming', 'my', 'again', 'everyone', 'one', 'hereby', 'while', 'three', 're', 'should', 'become', 'us', 'yet', 'to', '‚Äòve', 'if', 'these', 'seem', 'for', 'thereafter', 'will', 'was', 'or', 'beyond', 'several', 'once', 'say', 'such', 'towards', 'whereby', 'whom', 'first', 'noone', 'part', '‚Äòre', 'however', 'see', 'still', 'whereafter', 'hence', 'is', 'whatever', 'really', 'thereupon', 'but', 'fifty', 'another', 'hereupon', 'anything', 'might', 'down', 'front', 'back', 'former', 'your', 'nor', 'herein', 'third', 'up', 'something', 'at', 'name', 'others', 'eleven', 'empty', 'those', 'without', 'along', 'whoever', 'would', 'why', 'seemed', 'amount', 'side', 'seems', 'been', 'four', \"'m\", 'becoming', 'ours', 'six', 'sometimes', 'whither', '‚Äòm', '‚Äôm', 'between', 'had', 'them', 'her', 'so', 'nowhere', 'regarding', 'somehow', 'from', 'after', 'other'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34138e87-93f3-47e7-b8b9-bde391ef4500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04160fdc-44e0-4667-879a-682355f981cf",
   "metadata": {},
   "source": [
    "## How to CHeck‚úÖ How to ADD ‚ûï How to removed ‚ûñ Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "121c5110-9a2b-4173-8b37-f219c9eb26ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['pencil'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "436f286f-d8c4-454b-8673-778b5c30337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('btw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "626c939c-d191-4a3d-8b25-230f52ee6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['btw'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "43a65586-084c-4a29-b589-19754c5786a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c0950ca6-4695-4023-a347-dbc0f21aa5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e2a87d54-b292-48be-9190-1263d3019ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.remove('beyond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "59d4ddcf-4078-4db6-860b-2bdc41399ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['btw'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c9c4a-61de-4f7b-80ca-5bd112d01194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
